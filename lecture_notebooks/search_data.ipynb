{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ce5185",
   "metadata": {},
   "source": [
    "# Search Data and Movies\n",
    "\n",
    "This notebook shows how to study the power of Google Trends data to predict home sales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8920f973",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbdf1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdacd27d",
   "metadata": {},
   "source": [
    "I'm having path issues in VS Code, so I'm going to set my working directory to the main folder of the GitHub repo I'm working with. This will be different for you and for the class repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa88546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/libbyh/Documents/git/si313_instructors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255ec9b",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "## Google Trends\n",
    "[Google Trends](https://trends.google.com/home) data is publicly available. You can watch their tutorials (including [this intro video](https://youtu.be/G76OomPTrE0?si=yqEuTI3-51sP7FI5)) to learn more about how it works and how to use it.\n",
    "\n",
    "We're going to use Google Trends data as our independent variables.\n",
    "\n",
    "I've already downloaded data from the \"mortage rate\" topic for the last 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data and skip the extra header rows Google adds to the CSV file\n",
    "df_trends = pd.read_csv(\"lecture_notebooks/data/weekly-trends-mortgage-rates.csv\", skiprows=2)\n",
    "\n",
    "# Rename columns\n",
    "df_trends.columns = ['week_end', 'search_interest']\n",
    "\n",
    "# Replace value 30 with 40 in the 'Age' column\n",
    "df_trends['search_interest'] = df_trends['search_interest'].replace('<1', 0)\n",
    "\n",
    "# Convert 'points' column from object to float\n",
    "df_trends['search_interest'] = df_trends['search_interest'].astype(float)\n",
    "\n",
    "# Convert 'week_end' to datetime\n",
    "df_trends['week_end'] = pd.to_datetime(df_trends['week_end'])\n",
    "\n",
    "df_trends.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b11e2d",
   "metadata": {},
   "source": [
    "I got [weekly home sales data from Redfin](https://www.redfin.com/news/data-center/?msockid=1d0eb4848bd865901662a10d8acb6475). This file is big, so be patient if you load it. Pandas can read the compressed file, so you can leave it in the *.gz format you downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redfin = pd.read_csv(\"lecture_notebooks/data/weekly_housing_market_data_most_recent.tsv000.gz\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ae3f2",
   "metadata": {},
   "source": [
    "Redfin provides data at the county and metro level, and we need to aggregate up to the national level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6972b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the last 5 years of data\n",
    "df_redfin = df_redfin[df_redfin['PERIOD_END'] >= '2021-02-01'].copy()\n",
    "\n",
    "# Filter to county level only\n",
    "df_counties = df_redfin[df_redfin['REGION_TYPE'] == 'county'].copy()\n",
    "\n",
    "# Group by week and sum\n",
    "df_homesales = df_counties.groupby('PERIOD_END').agg({\n",
    "    'ADJUSTED_AVERAGE_HOMES_SOLD': 'sum'  # Sum across all counties\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "df_homesales.columns = ['week_end', 'homes_sold']\n",
    "\n",
    "# Trends are normalized to 0-100, so we need to do the same for homes sold\n",
    "df_homesales['homes_sold_scaled'] = 100 * (df_homesales['homes_sold'] - df_homesales['homes_sold'].min()) / (df_homesales['homes_sold'].max() - df_homesales['homes_sold'].min())\n",
    "\n",
    "# Convert date\n",
    "df_homesales['week_end'] = pd.to_datetime(df_homesales['week_end'])\n",
    "\n",
    "# Sort\n",
    "df_homesales = df_homesales.sort_values('week_end').reset_index(drop=True)\n",
    "\n",
    "print(f\"Created national dataset with {len(df_homesales)} weeks\")\n",
    "print(df_homesales.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac9f95b",
   "metadata": {},
   "source": [
    "Now we can merge the datasets so we have one with both `search_interest` and `home_sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392eb071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_trends, df_homesales, on='week_end', how='inner')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_comparison(df, date_col, var1, var2, \n",
    "                                 var1_label=None, var2_label=None,\n",
    "                                 title=None):\n",
    "    \"\"\"Plot two time series variables\"\"\"\n",
    "    \n",
    "    # Defaults\n",
    "    var1_label = var1_label or var1\n",
    "    var2_label = var2_label or var2\n",
    "    title = title or 'Time Series Comparison'\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    ax.plot(df[date_col], df[var1], linewidth=2, label=var1_label, alpha=0.8)\n",
    "    ax.plot(df[date_col], df[var2], linewidth=2, label=var2_label, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Scaled Index (0-100)', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e7d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_time_series_comparison(\n",
    "    df, 'week_end', 'homes_sold_scaled', 'search_interest',\n",
    "    var1_label='Homes Sold', \n",
    "    var2_label='Search Interest',\n",
    "    title='Home Sales vs. Search Interest Over Time'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ed88d",
   "metadata": {},
   "source": [
    "It looks like we don't have enough search interest data to use data before about 2023, so let's narrow our window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dde8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['week_end'] >= '2023-02-01'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c973ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_time_series_comparison(\n",
    "    df, 'week_end', 'homes_sold_scaled', 'search_interest',\n",
    "    var1_label='Homes Sold', \n",
    "    var2_label='Search Interest',\n",
    "    title='Home Sales vs. Search Interest Over Time'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2978627e",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "Now we'll analyze the relationship between search interest and home sales using the approaches from:\n",
    "- **Goel et al. (2010)**: Simple correlation and regression\n",
    "- **Yang et al. (2015)**: Autoregressive models (ARGO approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8db33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit simple linear regression model\n",
    "# Following Goel et al. approach: Y = β₀ + β₁X + ε\n",
    "\n",
    "# Prepare data\n",
    "X = df[['search_interest']]\n",
    "y = df['homes_sold_scaled']\n",
    "\n",
    "# Add constant (intercept) to model\n",
    "X_with_const = sm.add_constant(X)\n",
    "\n",
    "# Fit OLS model\n",
    "model_simple = sm.OLS(y, X_with_const).fit()\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 70)\n",
    "print(\"SIMPLE LINEAR REGRESSION: Home Sales ~ Search Interest\")\n",
    "print(\"=\" * 70)\n",
    "print(model_simple.summary())\n",
    "print()\n",
    "\n",
    "# Extract key statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Intercept (β₀): {model_simple.params['const']:.3f}\")\n",
    "print(f\"Slope (β₁): {model_simple.params['search_interest']:.3f}\")\n",
    "print(f\"  → For every 1-point increase in search interest,\")\n",
    "print(f\"     home sales increase by {model_simple.params['search_interest']:.3f} points (on 0-100 scale)\")\n",
    "print()\n",
    "print(f\"R-squared: {model_simple.rsquared:.4f}\")\n",
    "print(f\"  → Search interest explains {model_simple.rsquared*100:.1f}% of variance in home sales\")\n",
    "print()\n",
    "print(f\"F-statistic: {model_simple.fvalue:.2f} (p = {model_simple.f_pvalue:.2e})\")\n",
    "print(f\"  → Model is statistically significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a06c36",
   "metadata": {},
   "source": [
    "### Autoregressive Model (Yang et al. ARGO Approach)\n",
    "\n",
    "The key insight from Yang et al.'s ARGO paper: **recent past values are often the best predictor of near-future values**.\n",
    "\n",
    "We'll compare:\n",
    "1. **AR(1) Baseline**: Predict sales using only last week's sales\n",
    "2. **ARGO**: Predict sales using last week's sales + search interest\n",
    "\n",
    "Does search data improve predictions beyond simple autoregression?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a203441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged variables: use last week's home sales to predict this week\n",
    "df['homes_sold_lag1'] = df['homes_sold_scaled'].shift(1)\n",
    "\n",
    "# Remove rows with NaN (first week)\n",
    "df_ar = df[['homes_sold_scaled', 'homes_sold_lag1', 'search_interest']].dropna()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AUTOREGRESSIVE MODELS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Using {len(df_ar)} weeks of data (after removing NaNs from lagging)\")\n",
    "print()\n",
    "\n",
    "# Model 1: Just autoregression (AR baseline)\n",
    "X_ar = df_ar[['homes_sold_lag1']]\n",
    "y_ar = df_ar['homes_sold_scaled']\n",
    "X_ar_const = sm.add_constant(X_ar)\n",
    "\n",
    "model_ar = sm.OLS(y_ar, X_ar_const).fit()\n",
    "\n",
    "print(\"\\nMODEL 1: AR(1) Baseline - Sales(t) ~ Sales(t-1)\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"R-squared: {model_ar.rsquared:.4f}\")\n",
    "print(f\"  → Last week's sales explain {model_ar.rsquared*100:.1f}% of this week's sales\")\n",
    "print()\n",
    "\n",
    "# Model 2: Autoregression + Search Interest (ARGO approach)\n",
    "X_argo = df_ar[['homes_sold_lag1', 'search_interest']]\n",
    "X_argo_const = sm.add_constant(X_argo)\n",
    "\n",
    "model_argo = sm.OLS(y_ar, X_argo_const).fit()\n",
    "\n",
    "print(\"\\nMODEL 2: ARGO - Sales(t) ~ Sales(t-1) + SearchInterest(t)\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"R-squared: {model_argo.rsquared:.4f}\")\n",
    "print(f\"  → AR + Search explain {model_argo.rsquared*100:.1f}% of this week's sales\")\n",
    "print()\n",
    "\n",
    "# Calculate improvement\n",
    "r2_improvement = model_argo.rsquared - model_ar.rsquared\n",
    "pct_improvement = (r2_improvement / model_ar.rsquared) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DOES SEARCH DATA ADD PREDICTIVE POWER?\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"R² improvement: {r2_improvement:.4f}\")\n",
    "print(f\"Percentage improvement: {pct_improvement:.1f}%\")\n",
    "print()\n",
    "\n",
    "if r2_improvement > 0.01:\n",
    "    print(\"✓ YES: Search interest significantly improves prediction\")\n",
    "    print(f\"  Adding search data improves prediction by {pct_improvement:.1f}%\")\n",
    "else:\n",
    "    print(\"✗ NO: Search interest adds minimal predictive power\")\n",
    "    print(\"  Autoregression alone is nearly as good\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ARGO MODEL DETAILS:\")\n",
    "print(\"=\" * 70)\n",
    "print(model_argo.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b656ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home sales are highly seasonal (high in spring/summer, low in winter)\n",
    "# Let's add seasonal controls to our model\n",
    "\n",
    "# Create seasonal indicator variables\n",
    "df['spring'] = ((df['week_end'].dt.month >= 3) & \n",
    "                (df['week_end'].dt.month <= 5)).astype(int)\n",
    "df['summer'] = ((df['week_end'].dt.month >= 6) & \n",
    "                (df['week_end'].dt.month <= 8)).astype(int)\n",
    "df['fall'] = ((df['week_end'].dt.month >= 9) & \n",
    "              (df['week_end'].dt.month <= 11)).astype(int)\n",
    "# Winter (Dec-Feb) is the reference category\n",
    "\n",
    "# Recreate lagged analysis with seasonal controls\n",
    "df['homes_sold_lag1'] = df['homes_sold_scaled'].shift(1)\n",
    "df_seasonal = df[['homes_sold_scaled', 'homes_sold_lag1', 'search_interest',\n",
    "                   'spring', 'summer', 'fall']].dropna()\n",
    "\n",
    "# Model 3: ARGO + Seasonal Controls\n",
    "X_argo_seasonal = df_seasonal[['homes_sold_lag1', 'search_interest',\n",
    "                                 'spring', 'summer', 'fall']]\n",
    "y_seasonal = df_seasonal['homes_sold_scaled']\n",
    "X_argo_seasonal_const = sm.add_constant(X_argo_seasonal)\n",
    "\n",
    "model_argo_seasonal = sm.OLS(y_seasonal, X_argo_seasonal_const).fit()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 3: ARGO + Seasonal Controls\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"R-squared: {model_argo_seasonal.rsquared:.4f}\")\n",
    "print()\n",
    "\n",
    "# Compare to ARGO without seasonal controls\n",
    "print(f\"ARGO without seasons: R² = {model_argo.rsquared:.4f}\")\n",
    "print(f\"ARGO with seasons:    R² = {model_argo_seasonal.rsquared:.4f}\")\n",
    "print(f\"Improvement: {model_argo_seasonal.rsquared - model_argo.rsquared:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Seasonal coefficients:\")\n",
    "print(f\"  Spring vs Winter: {model_argo_seasonal.params['spring']:.3f} \" +\n",
    "      f\"(p={model_argo_seasonal.pvalues['spring']:.3f})\")\n",
    "print(f\"  Summer vs Winter: {model_argo_seasonal.params['summer']:.3f} \" +\n",
    "      f\"(p={model_argo_seasonal.pvalues['summer']:.3f})\")\n",
    "print(f\"  Fall vs Winter:   {model_argo_seasonal.params['fall']:.3f} \" +\n",
    "      f\"(p={model_argo_seasonal.pvalues['fall']:.3f})\")\n",
    "print()\n",
    "\n",
    "print(\"Search interest coefficient:\")\n",
    "print(f\"  Without seasonal controls: {model_argo.params['search_interest']:.3f}\")\n",
    "print(f\"  With seasonal controls:    {model_argo_seasonal.params['search_interest']:.3f}\")\n",
    "print()\n",
    "\n",
    "if abs(model_argo_seasonal.params['search_interest']) > abs(model_argo.params['search_interest']):\n",
    "    print(\"→ Controlling for seasons makes search MORE important\")\n",
    "else:\n",
    "    print(\"→ Some of search's predictive power was actually just seasonality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae60b48",
   "metadata": {},
   "source": [
    "# Why Doesn't Search Predict Housing Better?\n",
    "\n",
    "## Discuss:\n",
    "1. What makes housing more like music than movies?\n",
    "2. What would need to be true for search to predict housing sales better?\n",
    "3. What would we predict INSTEAD of sales that might work better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de795167",
   "metadata": {},
   "source": [
    "# More Discussion Questions\n",
    "\n",
    "Based on your analysis, discuss the following:\n",
    "\n",
    "## 1. Predictive Value\n",
    "- How much variance in home sales can be explained by search interest alone?\n",
    "- How does this compare to just using last week's sales (autoregression)?\n",
    "- Does adding search data meaningfully improve predictions?\n",
    "\n",
    "## 2. Temporal Dynamics\n",
    "- Based on the ARGO model, do searches seem to happen BEFORE sales, AT THE SAME TIME, or AFTER?\n",
    "- What does the coefficient on search interest tell us?\n",
    "- How might you test different time lags?\n",
    "\n",
    "## 3. Measurement Validity\n",
    "- Does \"mortgage rate\" search volume actually capture housing market activity?\n",
    "- Who might be over/under-represented in Google search data?\n",
    "- What other search terms might be better predictors?\n",
    "\n",
    "## 4. Comparison to Research Papers\n",
    "- Goel et al. found search data worked well for movies/video games but not music. Why might that be?\n",
    "- Yang et al. showed autoregression + search beat either alone for flu prediction. Did you find the same pattern?\n",
    "- What does this tell us about when \"digital trace data\" is valuable for prediction?\n",
    "\n",
    "## 5. Real-World Applications\n",
    "- Who would benefit from predicting home sales? (Real estate companies? Government? Banks?)\n",
    "- How accurate would predictions need to be to be useful?\n",
    "- What could cause this prediction model to break down over time? (Think: Google Flu Trends failure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b00f4",
   "metadata": {},
   "source": [
    "# When is Search Data Predictive?\n",
    "\n",
    "HIGH PREDICTIVE POWER:\n",
    "\n",
    "- Short time lag: Search → Action within days\n",
    "- Clear intent: Searching IS the start of the purchase action\n",
    "- Discrete action: Single transaction, not ongoing behavior\n",
    "- Limited alternatives: Searching Google is necessary part of process\n",
    "- Measurable outcome: Can track actual sales/events\n",
    "\n",
    "LOW PREDICTIVE POWER:\n",
    "\n",
    "- Long time lag: Months between search and outcome\n",
    "- Ambiguous intent: Just browsing, not buying\n",
    "- Complex process: Multiple steps, many decision points\n",
    "- Alternative discovery: Word-of-mouth, recommendations, habits\n",
    "- Diffuse outcomes: Hard to measure the actual behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbedf1",
   "metadata": {},
   "source": [
    "# What About other Time Lags?\n",
    "\n",
    "Maybe search interest predicts sales but not right away?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged search variables\n",
    "# Positive shift = search happens BEFORE sales (leading indicator)\n",
    "df['search_current'] = df['search_interest']                # Same week\n",
    "df['search_lag_1week'] = df['search_interest'].shift(1)     # 1 week ago\n",
    "df['search_lag_1month'] = df['search_interest'].shift(4)    # ~1 month ago (4 weeks)\n",
    "df['search_lag_3months'] = df['search_interest'].shift(13)  # ~3 months ago (13 weeks)\n",
    "df['search_lag_6months'] = df['search_interest'].shift(26)  # ~6 months ago (26 weeks)\n",
    "\n",
    "# Test correlation at different lags\n",
    "lags = {\n",
    "    'Current week (0)': 'search_current',\n",
    "    '1 week ago': 'search_lag_1week',\n",
    "    '1 month ago (4 weeks)': 'search_lag_1month',\n",
    "    '3 months ago (13 weeks)': 'search_lag_3months',\n",
    "    '6 months ago (26 weeks)': 'search_lag_6months'\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CORRELATION AT DIFFERENT TIME LAGS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Lag Period':<30} {'Correlation':<15} {'Sample Size':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "lag_results = {}\n",
    "for lag_name, lag_col in lags.items():\n",
    "    # Calculate correlation (dropna to handle missing values from shift)\n",
    "    valid_data = df[[lag_col, 'homes_sold_scaled']].dropna()\n",
    "    if len(valid_data) > 0:\n",
    "        corr = valid_data[lag_col].corr(valid_data['homes_sold_scaled'])\n",
    "        lag_results[lag_name] = corr\n",
    "        print(f\"{lag_name:<30} {corr:>6.4f}        n={len(valid_data)}\")\n",
    "    else:\n",
    "        print(f\"{lag_name:<30} {'N/A':<15} (insufficient data)\")\n",
    "\n",
    "# Find best lag\n",
    "best_lag = max(lag_results, key=lag_results.get)\n",
    "print()\n",
    "print(f\"Strongest correlation at: {best_lag} (r = {lag_results[best_lag]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictive power at different lags\n",
    "results_list = []\n",
    "\n",
    "for lag_name, lag_col in lags.items():\n",
    "    # Prepare data (drop NaN from shift)\n",
    "    analysis_df = df[['homes_sold_scaled', lag_col]].dropna()\n",
    "    \n",
    "    if len(analysis_df) > 30:  # Need enough data\n",
    "        X = sm.add_constant(analysis_df[[lag_col]])\n",
    "        y = analysis_df['homes_sold_scaled']\n",
    "        \n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        results_list.append({\n",
    "            'Lag': lag_name,\n",
    "            'R²': model.rsquared,\n",
    "            'Coefficient': model.params[lag_col],\n",
    "            'P-value': model.pvalues[lag_col],\n",
    "            'N': len(analysis_df)\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"REGRESSION RESULTS AT DIFFERENT LAGS\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5921d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation strength across different lags\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Test many lags (0 to 26 weeks)\n",
    "lag_weeks = range(0, 27)\n",
    "correlations = []\n",
    "\n",
    "for lag in lag_weeks:\n",
    "    df[f'temp_lag_{lag}'] = df['search_interest'].shift(lag)\n",
    "    valid = df[[f'temp_lag_{lag}', 'homes_sold_scaled']].dropna()\n",
    "    if len(valid) > 30:\n",
    "        corr = valid[f'temp_lag_{lag}'].corr(valid['homes_sold_scaled'])\n",
    "        correlations.append(corr)\n",
    "    else:\n",
    "        correlations.append(np.nan)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(lag_weeks, correlations, marker='o', linewidth=2, markersize=4)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=13, color='red', linestyle='--', alpha=0.3, label='3 months')\n",
    "ax.axvline(x=26, color='orange', linestyle='--', alpha=0.3, label='6 months')\n",
    "\n",
    "ax.set_xlabel('Lag (Weeks Before Sale)', fontsize=12)\n",
    "ax.set_ylabel('Correlation with Home Sales', fontsize=12)\n",
    "ax.set_title('Predictive Power of Search Interest at Different Time Lags', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Mark the peak\n",
    "peak_lag = np.nanargmax(correlations)\n",
    "peak_corr = correlations[peak_lag]\n",
    "ax.plot(peak_lag, peak_corr, 'r*', markersize=15, \n",
    "        label=f'Peak at {peak_lag} weeks (r={peak_corr:.3f})')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPeak correlation occurs at {peak_lag} week(s) lag\")\n",
    "print(f\"Correlation strength: r = {peak_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best ARGO model: Which lag of search works best with autoregression?\n",
    "\n",
    "df['homes_sold_lag1'] = df['homes_sold_scaled'].shift(1)\n",
    "\n",
    "# Test ARGO with different search lags\n",
    "argo_results = []\n",
    "\n",
    "for lag_name, lag_col in lags.items():\n",
    "    df_test = df[['homes_sold_scaled', 'homes_sold_lag1', lag_col]].dropna()\n",
    "    \n",
    "    if len(df_test) > 30:\n",
    "        X = sm.add_constant(df_test[['homes_sold_lag1', lag_col]])\n",
    "        y = df_test['homes_sold_scaled']\n",
    "        \n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        argo_results.append({\n",
    "            'Search Lag': lag_name,\n",
    "            'R²': model.rsquared,\n",
    "            'Search Coefficient': model.params[lag_col],\n",
    "            'Search P-value': model.pvalues[lag_col]\n",
    "        })\n",
    "\n",
    "argo_df = pd.DataFrame(argo_results)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ARGO MODEL WITH DIFFERENT SEARCH LAGS\")\n",
    "print(\"=\" * 70)\n",
    "print(argo_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af74897",
   "metadata": {},
   "source": [
    "### Time Lag Analysis Results\n",
    "\n",
    "**What we found:**\n",
    "- Strongest correlation at: ___ week lag\n",
    "- By 3 months lag: r = ___\n",
    "- By 6 months lag: r = ___\n",
    "\n",
    "#### Discuss\n",
    "1. Why does correlation decline as lag increases?\n",
    "2. What does this tell us about the housing decision timeline?\n",
    "3. Compare to movies - why is housing different?\n",
    "4. Does this support or challenge the idea that search predicts sales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a0c6f",
   "metadata": {},
   "source": [
    "If search \"predicted\" home purchases, we'd expect:\n",
    "- Strong correlation at 3-6 month lag (time to buy after searching)\n",
    "- But we find the opposite - correlation weakens!\n",
    "- This suggests: Search and sales are COINCIDENT, not predictive\n",
    "- Both are driven by the same seasonal/economic factors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
